{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89794686-7da2-48d3-a3e5-b1400319db7b",
   "metadata": {},
   "source": [
    "Starting a new machine learning project is always exciting, however, most beginners tend to overlook the importance of aquaring a good dataset. Usually, we are too focused on implementing a first prototype, figuring out which algorithm will result in the best accuracy, that we don't appreciate just how vital a good dataset is. \n",
    "\n",
    "Therefore, I would like to remind all data scientist this key piece of advice. \n",
    "\n",
    "**We do not rise to the complexity of our models, instead, we fall to the quality of the dataset** \n",
    "\n",
    "*note: (This is an adaptation of a quote by 650 BC greek soldier Archilochus, who said: We do not rise to the level of our expectations). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61047432-7454-4402-9fe8-f8ecd30429dc",
   "metadata": {},
   "source": [
    "So, data. For the most part, you will be confronted with one of the two situations: \n",
    "1. Use an existing dataset. \n",
    "2. Create your own dataset. \n",
    "\n",
    "\n",
    "Where to find datsets? Kaggle, tensorflow/keras, gobernment/ university sites, through papers, through competetions: imagenet, MOT challenge, etc. \n",
    "That's great if you are studying or doing reserach. However, more often than not, if you are working in the private sector, you will have to create you own datasets. That means either finding multiple datasets and combining them in an efficient way, setting up a pipeline to gather data, annotate data, clean data, preprocess data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49552a7-37da-4737-8a41-ec9f47fc45be",
   "metadata": {},
   "source": [
    "How to create your own dataset? \n",
    "Let's learn. There usually are two ways to go around it. \n",
    "1. You know what kind of data you want to work with, and you will figure out what kind of model to fit afterwards. \n",
    "2. You know what kind of problem you want to solve, and you will find data that allows you to do so. \n",
    "\n",
    "Since this is for demonstration purposes, we will create a versatile dataset. That can be used for multiple use cases. We will create a dataset containing many different data types, and it will be the dataset used throught the course to learn machine learning. \n",
    "\n",
    "So what kind of data should we use? There are many different types of data: it can be numerical, text, images, sound, time series... Possibilities are endless. Most tutorials out there focus mosty one either numerical datsets (a dataframe, sometimes with bools for True/False attributes) or pictures (think of MNIST, imagenet, COCO, etc). However, there are so many different types of data! In this tutorial, we will explore a broad variety. \n",
    "\n",
    "We will therefore create The Kaleidoscopic Zoo. Why? Well, besides creating posts about things that I am pasoniote about, during my free time I enjoy running. AT every starting line, coach Bennet is there with me, and throught many runs, I have picked up a habit from him. You see, coach Bennet LOVES himself a good dictionary. And that is a beatiful habit I've started to pick up. So why Kaleidoscopic? Well, if you look up the difinition, the first thing that comes is \"Having complex patterns of colours; multicoloured.\", not really what I'm going for. But if you continue and read the second definition, you will find \"Made up of a complex mix of elements; multifaceted.\" Now, that's the definition I want for our datatset. A dataset  made up of a complex mix of elements. A versatile dataset, containing different types of data, so it can be used for more than one use case. And Zoo, well, because its gonna be made up of animals. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05d159-3724-42da-a7cf-2ee25b2399ca",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f946b8-87e3-4e69-921d-d946fb3b6def",
   "metadata": {},
   "source": [
    "Inspo from https://towardsdatascience.com/how-to-create-and-tune-your-own-data-set-for-facial-recognition-using-neural-networks-8a68be38652\n",
    "icrawler doesn't seem to work, not sure its properly supported. However, scrappy seems to be a very popular alternative for webcrawlers\n",
    "Just will need to learn about it. But i think that it can defenetly be done. https://www.youtube.com/watch?v=s4jtkzHhLzY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e886d85b-0e27-45fb-8170-994d3ba59dd8",
   "metadata": {},
   "source": [
    "How to get data? Well, could defenetly open a browser and look for the data we want. We could then manually save everything. However, I hope that you will agree with me that that certanly isn't the most productive way of spending your time. And it's defenetly not a scalable way of obtaining data. \n",
    "Pherhaps if you need 100 data points, it'd be manageable. But, if you want to create a dataset contaning thousands of instances, this approach won't be efficient. \n",
    "So what can we do? Find a way of gathering data that doesn't rely on manual labor. Thankfully, that technology already exists. In order to create our dataset, we will be using web scrappers / web crawlers. Which is software design to extract data from websites. \n",
    "\n",
    "In this tutorial, we will be using scrapy https://scrapy.org/ You can install it via pip install scrapy or  conda install -c conda-forge scrapy. \n",
    "\n",
    "https://www.pexels.com/search/giraffe/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882505e1-6177-441e-93a7-885ce002bc9a",
   "metadata": {},
   "source": [
    "For pexels, you cant use a spider. robots.txt in the website prohibts it. However, they do have an api, so you can use that. This is an example on how to do it! \n",
    "https://olegkhomenko.medium.com/collecting-free-to-use-photos-from-pexels-using-python-and-pexels-api-81845aee67ef\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b1684-0ea8-44f2-91c3-49aee1c0bbe3",
   "metadata": {},
   "source": [
    "Run from terminal. You can use ! beofre command\n",
    "https://www.scrapingbee.com/blog/crawling-python/\n",
    "https://towardsdatascience.com/run-scrapy-code-from-jupyter-notebook-without-issues-69b7cb79530c\n",
    "https://www.mikulskibartosz.name/how-to-scrape-a-single-web-page-using-scrapy-in-jupyter-notebook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fd88b34-61c5-4c23-a810-b03026ca84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape webpage\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "# text cleaning\n",
    "import re\n",
    "# Reactor restart\n",
    "from crochet import setup, wait_for\n",
    "setup()\n",
    "\n",
    "class QuotesToCsv(scrapy.Spider):\n",
    "    \"\"\"scrape first line of  quotes from `wikiquote` by \n",
    "    Maynard James Keenan and save to json file\"\"\"\n",
    "    name = \"MJKQuotesToCsv\"\n",
    "    start_urls = [\n",
    "        'https://en.wikiquote.org/wiki/Maynard_James_Keenan',\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'ITEM_PIPELINES': {\n",
    "            '__main__.ExtractFirstLine': 1\n",
    "        },\n",
    "        'FEEDS': {\n",
    "            'quotes.csv': {\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"parse data from urls\"\"\"\n",
    "        for quote in response.css('div.mw-parser-output > ul > li'):\n",
    "            yield {'quote': quote.extract()}\n",
    "\n",
    "\n",
    "class ExtractFirstLine(object):\n",
    "    def process_item(self, item, spider):\n",
    "        \"\"\"text processing\"\"\"\n",
    "        lines = dict(item)[\"quote\"].splitlines()\n",
    "        first_line = self.__remove_html_tags__(lines[0])\n",
    "\n",
    "        return {'quote': first_line}\n",
    "\n",
    "    def __remove_html_tags__(self, text):\n",
    "        \"\"\"remove html tags from string\"\"\"\n",
    "        html_tags = re.compile('<.*?>')\n",
    "        return re.sub(html_tags, '', text)\n",
    "\n",
    "@wait_for(10)\n",
    "def run_spider():\n",
    "    \"\"\"run spider with MJKQuotesToCsv\"\"\"\n",
    "    crawler = CrawlerRunner()\n",
    "    d = crawler.crawl(QuotesToCsv)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "691f0cfa-70ee-447c-907c-09e247559663",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_spider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c6d7f49-43b7-414f-8d7e-ca85b35460e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class ScrapeSpider(scrapy.Spider):\n",
    "    name = 'spiderName'\n",
    "    start_urls = ['https://www.pexels.com/search/elephant/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        item = ImageItem()\n",
    "        if response.status == 200:\n",
    "            images = response.css(\"img.photo-item__img\")\n",
    "            #item['image_urls'] = self.url_join(rel_img_urls, response)\n",
    "            print(len(images))\n",
    "        return item\n",
    "\n",
    "    def url_join(self, rel_img_urls, response):\n",
    "        joined_urls = []\n",
    "        for rel_img_url in rel_img_urls:\n",
    "            joined_urls.append(response.urljoin(rel_img_url))\n",
    "\n",
    "        return joined_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7754cf98-03e4-4558-addb-a6704ad4c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@wait_for(10)\n",
    "def run_spider():\n",
    "    \"\"\"run spider with MJKQuotesToCsv\"\"\"\n",
    "    crawler = CrawlerRunner()\n",
    "    d = crawler.crawl(ScrapeSpider)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf34a2c2-d05d-4b97-a216-30cb52b8ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_spider()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8140c888-81cf-4a61-8fc7-d030b5401f85",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06129e1d-1bf0-4604-9b4a-e0cd16b883ef",
   "metadata": {},
   "source": [
    "https://a-z-animals.com/animals/elephant/\n",
    "Here there are two important things: \n",
    "1. \"div.animal-facts-box\"\n",
    "2. \"div.single-animal-text\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b9f75-db44-4af5-8d26-fb4e62f337c9",
   "metadata": {},
   "source": [
    "We can use a crawler with the text!!! it conects !!! 200 means connects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3cf63-42c1-41fc-b564-f1aa7985ae18",
   "metadata": {},
   "source": [
    "## Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d26ad-8006-40fb-ae4b-a4f25d5e1083",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://a-z-animals.com/animals/elephant/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c78a05-1259-4577-8c8a-b30f49bbe8bd",
   "metadata": {},
   "source": [
    "## Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cddd54e-6133-4077-8dc0-a040509ad162",
   "metadata": {},
   "source": [
    "https://www.pexels.com/videos/\n",
    "https://www.pexels.com/search/videos/giraffe/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b609e-2eab-4e35-baa7-ee48244144f5",
   "metadata": {},
   "source": [
    "## Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a3c27-2d16-483c-9da2-1e27b66f07d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.zapsplat.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe0b42e-a132-4c6d-985b-f8a89ac22068",
   "metadata": {},
   "source": [
    "## Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6c2d02-7362-4007-9520-a972dbffd07d",
   "metadata": {},
   "source": [
    "Stocks, cant really do it with animals. couldnt find records of animal population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5defcf8b-fcc3-4896-a8e5-c589fa28d57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a3519-f6cf-4c7e-bfed-e57fb51b80bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca2930-30aa-440f-a612-c67351c1fbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc3355-3712-49e6-b044-7de698ca7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "The animals are gonna be: \n",
    "    \n",
    "    Elephant\n",
    "    Lion\n",
    "    Alligator \n",
    "    Bat \n",
    "    Bear\n",
    "    Dog\n",
    "    Frog\n",
    "    Cricket\n",
    "    Bird\n",
    "    Sheep \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLOps]",
   "language": "python",
   "name": "conda-env-MLOps-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
